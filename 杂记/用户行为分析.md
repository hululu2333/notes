### 用户行为分析

> 用户访问公司的网页时，会产生一些埋点日志。这些日志存在后端服务器上。
>
> 我要做的是去后端服务器上获取这些埋点日志，对其进行解析、分析，将最终分析的结果存进mongodb，供其他人使用
>
> 项目地址：https://github.com/hululu2333/user_behaviour_analysis



#### 大致流程

> 在埋点日志所在的节点上启动一个flume，用taildir监控日志文件夹，将采集到的日志发送到hdfs集群所在集群的某个节点的某个端口
>
> 启动第二个flume，接受第一个flume发来的日志，并将这些日志按照日期存放在hdfs不同文件夹中
>
> 编写一个shell脚本，通过crontab使其每天凌晨运行。每次运行时先调用beeline和load将前一天的日志导入hive，hive表以日期静态分区。随后调用spark程序，spark程序接收一个日期，并对这个日期的hive中的数据进行解析分析，将最终结果导入mongodb



#### 遇见的问题

> **taildir监控日志：**假如12月28日的日志名为2020-12-28.log。当这个文件到了一定大小时，这个文件会更名为2020-12-28.log.1，然后创建一个新的2020-12-28.log文件用来放新的数据。按理说flume应该会去拉取新出现的文件名2020-12-28.log.1的数据，但经过测试发现并不会，不确定是哪一处做了优化。
>
> **load数据到hive时数据不完整：**假如hdfs上有5m数据，我们将其load到hive中。我们去到hive在hdfs上的储存路径看，发现这个文件只有500B了。去hive表中查询，确实只有几条数据。但是当我们试图把这个文件下载下来时，发现下载下来的文件确实有5m，然后我们将下载下来的文件替换hive储存在hdfs中的那个文件，一切都正常了，文件大小正常显示为5m，hive表中也能查到正确的数据
>
> 我猜测是load时出现一些问题，只将一部分数据加载到了hive中。但我们hive使用的是内部表，必须将文件转移到hive的warehouse中，所以导致虽然这个文件有那么大，但是对外显示比较小，对外显示的大小就是hive事实上读取的数据的大小
>
> 为什么会出现这个问题还不确定，每次通过定时任务调度时才会出现，我猜测是用定时任务时加了个黑洞
>
> **DataSet写入mongodb：**导入相关依赖，依赖见源码，然后调用DataSet的方法finance_analysis.write().mode("append").format("com.mongodb.spark.sql").option("spark.mongodb.output.uri", url3).option("spark.mongodb.input.uri", url3).save();





#### 源码

> 埋点日志端flume配置

``` xml
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
 
# source相关配置
a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
# JSON格式的文件，用于记录每个拖尾文件的索引节点，绝对路径和最后位置
a1.sources.r1.positionFile = /home/ziyun/zz/flume_source/taildir_position.json
# 文件组列表
a1.sources.r1.filegroups = f1
# 监控的文件组的绝对路径
a1.sources.r1.filegroups.f1 = /ziyun/deploy/nodeServer/analysis/log.*
a1.sources.r1.maxBatchCount = 1000

# channel相关配置
a1.channels.c1.type = file
# 数据存放路径
a1.channels.c1.dataDirs = /home/ziyun/zz/flume_channel/datadir
# 检查点路径
a1.channels.c1.checkpointDir = /home/ziyun/zz/flume_channel/checkpointdir
# channel中最多缓存多少
a1.channels.c1.capacity = 2000
# channel一次最多吐给sink多少
a1.channels.c1.transactionCapacity = 500
a1.channels.c1.keep-alive = 10

# sink相关配置
a1.sinks.k1.type = avro
a1.sinks.k1.channel = c1
a1.sinks.k1.hostname = 192.168.0.105
a1.sinks.k1.port = 44321
```



> hdfs端flume配置

``` xml
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
 
# source相关配置
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 192.168.0.105
a1.sources.r1.port = 44321
# 自定义拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.ziyun.user_behavior_analysis.service.LogHandle$Builder

# channel相关配置
a1.channels.c1.type = file
# 数据存放路径
a1.channels.c1.dataDirs = /root/zz/flume_channel/datadir
# 检查点路径
a1.channels.c1.checkpointDir = /root/zz/flume_channel/checkpointdir
# channel中最多缓存多少
a1.channels.c1.capacity = 2000
# channel一次最多吐给sink多少
a1.channels.c1.transactionCapacity = 500
a1.channels.c1.keep-alive = 10

# sink相关配置
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs:///flume/%Y-%m-%d
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = logs-
a1.sinks.k1.hdfs.fileSuffix = .log
#是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
a1.sinks.k1.hdfs.roundUnit = day
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
#a1.sinks.k1.hdfs.batchSize = 500
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#a1.sinks.k1.hdfs.codeC = SnappyCodec
#多久生成一个新的文件 单位是秒
a1.sinks.k1.hdfs.rollInterval = 0
#设置每个文件的滚动大小  128M
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 0
#文件副本数
a1.sinks.k1.hdfs.minBlockReplicas = 1
a1.sinks.k1.channel = c1
```



> 把数据加载到hive并执行spark程序的脚本

``` bash
待完善
```



> pom文件

``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>user_behaviour_analysis</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.4.0</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.4.0</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.0</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.4.0</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_2.11</artifactId>
            <version>2.4.0</version>
        </dependency>

        <dependency>
            <groupId>org.mongodb.spark</groupId>
            <artifactId>mongo-spark-connector_2.11</artifactId>
            <version>2.4.0</version>
        </dependency>

        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.16</version>
        </dependency>

        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>2.8.5</version>
        </dependency>

        <dependency>
            <groupId>org.mongodb.spark</groupId>
            <artifactId>mongo-spark-connector_2.11</artifactId>
            <version>2.4.0</version> <!-- 2.2.0会有Mongo游标异常的bug-->
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.4</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>


</project>
```



> 程序入口：BehaviourAnalysis

``` java
import com.hu.parser.MyJsonParser;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.ArrayList;

/**
 * 执行用户行为分析功能
 * 每天被调用一次，被调用时从hive中读取昨天的埋点日志
 * 经过分析后，将分析结果放进mongodb
 */
public class BehaviourAnalysis {
    public static void main(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .appName("Java Spark Hive Example")
                .master("local[*]")
                .enableHiveSupport()
                .getOrCreate();
        spark.sparkContext().setLogLevel("ERROR");


        //从hive中读出一张表，并以二维数组的形式储存，Row底层应该是数组实现，所以我们可以把Row[]看作二维数组
        Dataset<Row> ds = spark.sql("select * from user_behavior_analysis.ods_pbox_log where dt = '"+args[0]+"'");
        System.out.println("从hive表中读出的原始json数据：");

        try{ //这个trycatch解决当天没有埋点日志的问题
            ds.show(50);
        }catch (Exception e){
            System.out.println("当天可能没有任何数据，或者发生一些其他从hive表中读取原始json数据产生的异常。");
            System.out.println("程序非正常退出");
            return;
        }


        Row[] rows = (Row[]) ds.collect();

        //取出andon产生的埋点日志。日志中包含"project":"andon"
        String para = "\"project\":\"andon\"";
        ArrayList<String> andons = new ArrayList<>();
        System.out.println("属于andon的json原始json数据:");
        for (Row row : rows) {
            if (row.get(0).toString().contains(para)) {
                andons.add(row.get(0).toString());
                System.out.println(row.get(0).toString());
            }
        }

        if(andons.size()==0){
            System.out.println("今天没有andon的json日志，直接退出");
            return;
        }

        //解析json，解析为一张二维表。我们的用户行为分析就是在这张二维表上进行
        Dataset data = MyJsonParser.parse(spark, andons);

        //创建一个用来查询的临时表
        data.createOrReplaceTempView("temp");


        //这一处的代码用来分析企业页面情况，生成的数据导入finance_andon_company_page_analysis表
        //sql2：获得所有公司的名字
        String sql2 ="select companyName from temp group by companyName";
        Row[] companyNames = (Row[]) spark.sql(sql2).collect();
        for(Row row : companyNames){
            String companyName = (String) row.get(0);
            //System.out.println("companyName is :"+companyName);
            //sql3：拿到每个公司点击每个网页的次数和最新时间，同一个公司点同一个链接，除了Date字段，其他都是一样的。但是Date字段只要求精确到日，那没问题了，都一样
            String sql3 = "select first(project) project, first(companyName) companyName, SUBSTR(max(Date),1,10) Date, first(page) page, href, count(Date) click_count, max(Date) click_last_time from temp where companyName = '"+companyName+"' group by href";
            Dataset company_analysis = spark.sql(sql3); //每个company_analysis中包含了一个公司当天的情况

            String url1 = "mongodb://192.168.0.175:27117/ziyun-iot.finance_andon_company_page_analysis";
            company_analysis.write().mode("append").format("com.mongodb.spark.sql").option("spark.mongodb.output.uri", url1).option("spark.mongodb.input.uri", url1).save();
        }


        //这一处的代码用来分析用户页面情况，生成的数据导入finance_andon_user_page_analysis表
        //sql4：获得所有用户的ssoId
        String sql4 ="select ssoId from temp group by ssoId";
        Row[] ssoids = (Row[]) spark.sql(sql4).collect();
        for(Row row : ssoids){
            String ssoid = (String) row.get(0);
            //System.out.println("ssoid is :"+ssoid);
            //sql5：拿到每个用户点击每个网页的次数和最新时间，同一个用户点击同一个页面的情况下，除了Date值，其他都是一样的。但Date字段只取到日，所以不需要和其他表合并
            String sql5 = "select first(project) project, first(ssoId) ssoId, first(openId) openId, first(unionId) unionId, first(companyName) companyName, SUBSTR(max(Date),1,10) Date, first(page) page, href, count(Date) click_count, max(Date) click_last_time from temp where ssoId = '"+ssoid+"' group by href";
            Dataset user_analysis = spark.sql(sql5); //每个company_analysis中包含了一个公司当天的情况
            //user_analysis.show(); //这一行代码今后替换为：将这个DataSet的数据放进目标表

            String url2 = "mongodb://192.168.0.175:27117/ziyun-iot.finance_andon_user_page_analysis";
            user_analysis.write().mode("append").format("com.mongodb.spark.sql").option("spark.mongodb.output.uri", url2).option("spark.mongodb.input.uri", url2).save();
        }


        //这一处的代码用来分析用户使用情况，生成的数据导入finance_andon_user_analysis表
        //sql6：获取所有需要的信息
        String sql6 = "select userName, userType, isAdmin, isPersonInCharge, ssoId, project, openId, unionId, companyName, SUBSTR(Date,1,10) Date, isRegister from temp";
        Dataset finance_analysis = spark.sql(sql6);

        String url3 = "mongodb://192.168.0.175:27117/ziyun-iot.finance_andon_user_analysis";
        finance_analysis.write().mode("append").format("com.mongodb.spark.sql").option("spark.mongodb.output.uri", url3).option("spark.mongodb.input.uri", url3).save();


        //展示完整的表，用来对照
        //spark.sql("select * from temp").show();

    }
}
```



> 解析类：MyJsonParser

``` java
import com.google.gson.JsonObject;
import com.google.gson.JsonParser;
import com.hu.util.HuUtils;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import scala.util.parsing.json.JSONArray;
import scala.util.parsing.json.JSONObject;

import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashSet;

/**
 * 对json进行解析的一个工具类
 */
public class MyJsonParser {
    private MyJsonParser(){

    }

    /**
     * 输入一个json字符串数组，将每个json字符串解析后，返回一个Row,
     * 一个Row中包含了一个json的所有字段的值
     * 最终这些Row合并为一个DataSet，并返回
     */
    public static Dataset<Row> parse(SparkSession spark, ArrayList<String> strs){
        JsonParser parser = new JsonParser(); //google提供的json解析器
        String header = "page,cip,cid,cname,userName,deviceRatio,colorDepth,devicePixelRatio,isPersonInCharge," +
                "isRegister,ssoId,project,token,href,userType,to,openId,unionId,companyName,lang," +
                "from,Date,isAdmin"; //dataSet中的列名
        ArrayList<String> datalist = new ArrayList<>(); //字符串数组，每个字符串是一个row

        //取出历史ssoId表
        Dataset ssoIdTable = spark.sql("select * from user_behavior_analysis.registered_user");
        ArrayList<String> ssoIdList = HuUtils.getColFromDs(ssoIdTable,0);
        System.out.println("所有在注册表中的ssoId");
        ssoIdList.forEach(ssoId -> System.out.println(ssoId));
        HashSet<String> willRegister = new HashSet<>();

        //将json字符串数组转换为解析后的字符串数组
        for(String str : strs){

            StringBuffer row = new StringBuffer(); //看作是表中的一行数据
            JsonObject json = (JsonObject) parser.parse(str);

            //拿到这个json的每个字段的值
            if(json.has("page")){
                row.append(json.get("page").toString()).append(",");
            }else{
                row.append("").append(",");
            }

            //二次解析，将citySn这个字段的值再进行解析
            if(json.has("citySN")){
                JsonObject json1 = (JsonObject) parser.parse(json.get("citySN").toString());

                if(json1.has("cip")){
                    row.append(json1.get("cip").toString()).append(",");
                }else{
                    row.append("").append(",");
                }

                if(json1.has("cid")){
                    row.append(json1.get("cid").toString()).append(",");
                }else{
                    row.append("").append(",");
                }

                if(json1.has("cname")){
                    row.append(json1.get("cname").toString()).append(",");
                }else{
                    row.append("").append(",");
                }

            }else{
                row.append("").append(",").append("").append(",").append("").append(",");
            }


            row.append(json.get("userName").toString()).append(",");
            row.append(json.get("deviceRatio").toString()).append(",");
            row.append(json.get("colorDepth").toString()).append(",");
            row.append(json.get("devicePixelRatio").toString()).append(",");
            row.append(json.get("isPersonInCharge").toString()).append(",");

            //判断以前是否存在这个ssoId
            String ssoId = json.get("ssoId").toString();
            ssoId = ssoId.replace("\"","");
            if (ssoId.equals("")){
                ssoId="nall";
            }
            String isRegister = "";
            if(ssoIdList.contains(ssoId)){
                System.out.println(ssoId+"在已注册表中");
                isRegister = "0";
            }else{
                System.out.println(ssoId+"不在表中");
                isRegister = "1";
                //将这个ssoId加入到准注册者队列中，等程序运行完再将他们加入到以注册表中
                willRegister.add(ssoId);
                //spark.sql("insert into user_behavior_analysis.registered_user values("+ssoId+")");
            }
            row.append(isRegister).append(",");
            row.append(ssoId).append(",");



            row.append(json.get("project").toString()).append(",");
            row.append(json.get("token").toString()).append(",");
            row.append(json.get("href").toString()).append(",");
            row.append(json.get("userType").toString()).append(",");
            row.append(json.get("to").toString()).append(",");

            if(json.has("openId")){
                row.append(json.get("openId").toString()).append(",");
            }else{
                row.append("").append(",");
            }

            if(json.has("unionId")){
                row.append(json.get("unionId").toString()).append(",");
            }else{
                row.append("").append(",");
            }

            if(json.has("companyName")){
                row.append(json.get("companyName").toString()).append(",");
            }else{
                row.append("").append(",");
            }

            row.append(json.get("lang").toString()).append(",");
            row.append(json.get("from").toString()).append(",");

            //Data的值是个时间戳，我们要把他换成yyyy-MM-dd HH:mm:ss格式
            //时间戳解析器
            SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
            long time = new Long(json.get("Date").toString());
            String result = simpleDateFormat.format(new Date(time));
            row.append(result).append(",");
            row.append(json.get("isAdmin").toString());

            System.out.println("第一行解析后的数据为："+row.toString());
            datalist.add(row.toString());
        }

        //构建DataSet
        Dataset<String> datasetS = spark.createDataset(datalist, Encoders.STRING());
        System.out.println("将一个List转化为一个原始的DataSet，只有一列");
        datasetS.show();

        System.out.println("将这个只有一列的DataSet以csv的格式读进新的DataSet，默认以’，‘为分隔符，将一列转化为多列");
        System.out.println("然后调用toDF方法，给这多列数据的每一列加一个列名");
        Dataset dataset = spark.read().csv(datasetS).toDF(header.split(","));
        System.out.println("今天的数据，解析后的结果为：");
        dataset.show();

        //新注册的加入注册表
        willRegister.forEach(ssoId -> {
            System.out.println("新的ssoId,即将加入hive的注册表"+ssoId);
            spark.sql("insert into user_behavior_analysis.registered_user values('"+ssoId+"')");
        });

        return dataset;
    }
}
```

